
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Engineering Health</title>
  <meta name="author" content="Cerner">

  
  <meta name="description" content="">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://engineering.cerner.com/blog/page/5">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/site/site.css" media="screen, projection" rel="stylesheet" type="text/css">

  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <script src="/javascripts/libs/bootstrap.min.js"></script>

  <link href="/atom.xml" rel="alternate" title="Engineering Health" type="application/atom+xml">
  
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37701128-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body  class="top-navbar  ">
  <header id="banner" class="navbar navbar-fixed-top" role="banner">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">
        <img src="/images/logo2.png" title="Cerner Engineering Health">
      </a>

      <nav id="nav-main" class="nav-collapse" role="navigation">
  <ul id="menu-top-menu" class="nav pull-right">
    <li class="menu-life-as-an-engineer">
      <a href="/about">Life as an Engineer</a>
    </li>
    <li class="menu-who-we-are">
      <a href="/engineers">Who We Are</a>
    </li>
    <li>
      <a href="/blog/archives">Archives</a>
    </li>
  </ul>      
</nav>

    </div>
  </div>
</header>

<div class="container">
  <div class="row">
    <div class="span8 pull-left page-header">
      <img class="site-header__img" src="/images/skyline2.png" alt="">    
    </div>

    
      <div class="span4 pull-right"> 
        <div id="search-form-wrapper">
          <form class="form-search" id="searchform" role="search" action="https://google.com/search" method="get" >
            <label class="hide" for="q">Search for:</label>
            <input name="q" type="hidden" value="site:engineering.cerner.com" />
            <input class="search-query" id="q" name="q" type="text" results="0" placeholder="Search">
            <input class="btn" id="searchsubmit" type="submit" value="Search" >
          </form>      
        </div>
      </div>
    
  </div>
</div>

  
  <div class="container">
    <div class="row-fluid">
      <div id="main" class="span8" role="main">
  
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/03/cerner-and-tycho/">Cerner and Tycho</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/jonny-wright"><span class="fn">Jonny Wright</span></a></span>
 | 








  


<time datetime="2013-03-27T00:00:00-05:00" pubdate data-updated="true">Mar 27<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><h2>Introduction</h2>

<p><a href="http://eclipse.org/tycho/">Tycho</a> is a build tool for integration between <a href="http://www.eclipse.org/pde/">PDE</a> and <a href="http://maven.apache.org/">Maven</a>.  Cerner has a long history of working with <a href="http://www.eclipse.org/home/categories/rcp.php">Eclipse RCP</a> but <a href="http://www.eclipse.org/equinox/p2/">P2</a> integration is something of a more recent phenomenon.  This post is going to talk about how Cerner is using Tycho, what prompted our transition to Tycho and how we accomplished moving our build to using Tycho.</p>

<h2>Why we use Tycho</h2>

<p>The first question when evaluating any tool is asking yourself: &ldquo;why would I use that&rdquo;?  In our case, Cerner already has an extensive Maven ecosystem.  For years it has been common practice, when developing Java, to use Maven as the go to build, site, and deployment tool.  What we needed was a set of extensions that allowed us to use all the facets of Maven while creating PDE artifacts.  Traditionally, the set of PDE artifacts that we needed to create and manage was limited to products, targets, and bundles.  Using the Maven conventions, we could build out all of these PDE artifacts while allowing the Maven POM to be the single source of truth for most build projects.  This worked well enough until we needed to rework our assemblies to be comprised of features, inserting another PDE artifact into the build lifecycle.</p>

<p>When we first began RCP development, no tooling existed to help us with creating PDE artifacts through Maven, so we created our own.  When the time came to start integrating additional PDE concepts, so that we could begin the path of integrating with P2 director, there was Tycho.</p>

<h2>How we use Tycho</h2>

<p>We use tycho generally to build the following set of PDE artifacts:</p>

<ul>
<li>Products</li>
<li>Features</li>
<li>Mirrors + Targets</li>
<li>Mirrors</li>
</ul>


<p>While the mirrors are not technically Eclipse artifacts, we use Tycho to mirror the Eclipse (for now Juno) repositories so that our builds are not hammering the Eclipse update site.  When you have 10&rsquo;s to 100&rsquo;s of users updating targets and running builds, it is just common courtesy to have your own mirror.</p>

<h3>Features</h3>

<p>We began our journey with Tycho by using it to generate P2 repositories from feature definitions.  The bulk of the heavy lifting is accomplished through the use of the tycho-packaging-plugin (as well as the base plugin, tycho-maven-plugin).  Since we deploy all of our plugins through maven, it is ideal to have Maven (Tycho in this case) do the dependency resolution for us and fill out the contents of our feature.  It also does all the work of turning that feature into a p2 repository.</p>

<p>This was actually a nontrivial exercise for us.  While we could have used a single feature definition, we tightly controll the dependencies that are pulled into our assembly.  Therefore, as we looked to breakdown our feature definition it was more complicated than simply pulling in the Juno feature and heaping everything into a feature on top of that.  Since we only use a subset of the Eclipse platform, we only want to pull forward the pieces we use explicitly (we also ran into some rather strange behavior with the Eclipse Jetty features).  We wanted to define the features in such a way that consumers of our platform could select a single feature that represented our core platform + Eclipse (as well as layer in additional functionality related to building their application).  In the end, we defined a set of about 20 features that rolled into a single working repository.  This way, consumers only needed to know the location of a single repository and then could use the metadata in the features to determine which features they needed.</p>

<h3>Products</h3>

<p>Starting at features made sense because there was no existing infrastructure for us to overhaul to build out the features.  The products were a different story.  With the products, we had an entire set of plugins that managed which version of Eclipse was pulled in for the product, how the .product was generated, and how we rolled that into an .exe.</p>

<p>The extra time we took in order to define the features thoroughly actually made this process significantly easier.  Today, we build a set of 8 applications:</p>

<ul>
<li>CareAware Criticalcare Dashboard</li>
<li>CareAware Criticalcare Personalized</li>
<li>CareAware Infusion Management Dashboard</li>
<li>CareAware Infusion Management Personalized</li>
</ul>


<p>Each of these products also has a corresponding mock build (where we substitute in mock resources to populate data for our views).  The definitions of these products was actually very simple and each ended up being around 8-10 features to create the product.  With the aggregated repository, we had a single repository in each POM where the versions for the features were determined and filled out.  Using the tycho-packaging-plugin and the tycho-p2-director plugin, we were able to automate the build of the corresponding p2 repositories which resulted from the Tycho builds.</p>

<h3>Mirrors + Targets</h3>

<p>Being the kind citizens that we are, we didn&rsquo;t want to abuse the Eclipse p2 release repository (also we didn&rsquo;t want them to cut us off like Maven Central).  To be good citizens in this build world, we decided we needed to mirror the Eclipse repositories ourselves.  Luckily for us, Tycho developers had already anticipated that need!  By using the <a href="http://wiki.eclipse.org/Tycho/Additional_Tools">tycho-p2-extras-plugin</a>, we were able to easily automate the mirroring of the required features from the Eclipse release repository.  Score (another) 1 for Tycho.</p>

<p>As part of the process for deploying these mirrors, we actually used the build-helper-plugin to deploy .target files to point to the mirrors into our maven repository.</p>

<h3>Why not bundles?</h3>

<p>The all important question: Why didn&rsquo;t we do bundles?  The two biggest things stopping us from uplifting our bundles was the lack of an easy way to incorporate non bundle sources into builds and the difficulty in generating sites with code coverage for projects that use both Java and Groovy (a sad state of affairs I don&rsquo;t recommend).  In the end, these hurdles plus the amount of time it would take to uplift our own projects to match the expected build structure was a significant overhaul.  We decided that, for now, the maven-bnd-plugin and m2eclipse can support our needs well enough for bundle development.</p>

<h2>From Custom Maven To Tycho</h2>

<p>So I&rsquo;ve mentioned throughout this blog post about how we moved from custom maven to tycho; so what does that mean exactly?  Back at the dawn of time (as I reckon it, others may know it as 2007), Cerner set down the path of doing Eclipse RCP development.  At the time, there weren&rsquo;t many good tools to automate your PDE builds.  Until very recently, most of the Eclipse projects have used Ant as their build system.  For our ecosystem and needs, it was Maven or bust.  So we set about creating plugins that would help us at all levels of the build.  Maven plugins were created for all of the following actions:</p>

<ul>
<li>Creating prodcuts</li>
<li>Creating Manifests for bundles</li>
<li>Running OSGi tests</li>
<li>Generating OSGi diff reports (on the manifests)</li>
<li>Accessing mirrored Eclipse instances</li>
<li>Generating target platforms based on POM configuration</li>
</ul>


<p>This turned out to be a rather extensive amount of work.  The model worked relatively successfully until a consumer came along with a set of requirements that read an awful lot like: &ldquo;we&rsquo;d like P2 integration included if we move to the iAware platform&rdquo;.  This threw a wrench in our system, because while P2 can work (and does) work by simply using .product files to define content sets, we had no tooling to generate P2 repositories.  We also had no mechanisms that would allow us to update only parts of their application (since this was 5 development teams working in tandem to produce a single assembly).  When all of this converged, we realized our build system had become insufficient and it was time to explore alternatives.  As you can see from the path we took, we started with the least intrusive changes and scaled up from there.  As of this blog, iAware is preparing for the first release using features and products.</p>

<h2>How do we deploy?</h2>

<p>Get ready for one ugly POM configuration:</p>

<p><img class="center" src="/assets/2013-03-27-cerner-and-tycho/P2-PomConfiguration-Edit2.png" title="P2 POM Configuration" ></p>

<p>If that isn&rsquo;t enough to scare you, I don&rsquo;t know what is.  I wrote it and it terrifies me.  What it really means though is that I haven&rsquo;t found a great way for deploying p2 repositories as part of the maven build process.  Options exist, such as deploying the zip file of the repository into the maven repo or writing shell scripts to automate the deployment, but neither of these sat well with me.  So I went for the least distasteful choice and wrote some basic ant scripts :).</p>

<p>All in all, Tycho has become a large part of our development ecosystem.  This blog only really touches the surface, but Tycho has become integral at all stages (besides bundle dev) to managing our assemblies.  So, thank you Tycho contributors!</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/near-real-time-processing-over-hadoop-and-hbase/">Near Real-time Processing Over Hadoop and HBase</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/ryan-brush"><span class="fn">Ryan Brush</span></a></span>
 | 








  


<time datetime="2013-02-27T00:00:00-06:00" pubdate data-updated="true">Feb 27<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><h2>From MapReduce to realtime</h2>

<p>This post covers much of the <a href="http://na.apachecon.com/schedule/presentation/161/">Near-Realtime Processing Over HBase</a> talk I’m giving at <a href="http://na.apachecon.com/">ApacheCon NA 2013</a> in blog form. It also draws from the <a href="http://strataconf.com/stratany2012/public/schedule/detail/25387">Hadoop, HBase, and Healthcare</a> talk from StrataConf/Hadoop World 2012.</p>

<p>The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics. For instance, if a user searches for &ldquo;heart disease&rdquo; and a patient has &ldquo;myocardial infarction&rdquo; documented, that document should be highly ranked in the results.</p>

<p>Analyzing and semantically annotating can be computationally expensive, especially when building indexes that could grow into the billions. Algorithms in this space may be discussed in a future blog post, but for now we focus on creation of an infrastructure up to the computational demands. For this, Hadoop is a great fit. A search index is logically a function of a set of input data, and MapReduce allows us to apply such functions in parallel across an arbitrarily large data set.</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/chart-search-screen.png" title="Chart Search" ></p>

<h4>A trend towards competing needs</h4>

<p>The above pattern is powerful but creates a nice problem to have: people want the output of the processing &mdash; in this case, updates to search indexes &mdash; faster. Since we cannot run a MapReduce job over our entire data set every millisecond, we encounter competing needs; the need to <em>process all data holistically</em> conflicts with the need to <em>quickly apply incremental updates</em> to that processing.</p>

<p>This difference may seem simple, but has deep implications.  For instance:</p>

<ul>
<li><p>With MapReduce we can move our computation to the data, but fast updates require moving data to computation.</p></li>
<li><p>MapReduce jobs produce output as a pure function of the input; realtime processing needs to handle outdated state. For instance, we build a phone book and a name changes from Smith to Jones, realtime processing must remove the outdated entry, whereas MapReduce simply rebuilds the whole phone book.</p></li>
<li><p>MapReduce jobs often assume a static set of complete data, whereas realtime processing may see partial data or new data introduce in an unexpected order.</p></li>
</ul>


<p>And despite these differences, our processing output must be the identical; we need to apply the same logic across very different processing models.</p>

<h2>Realtime and batch layers</h2>

<p>These significant differences mean different processing infrastructures. Nathan Marz described this well in his <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">How to Beat the CAP Theorem</a> post. The result is a system that uses complementary technologies: stream-based processing with <a href="http://storm-project.net/" title="">Storm</a> and batch processing with Hadoop.</p>

<p>Interestingly, <a href="http://hbase.apache.org/" title="">HBase</a> sits at a juncture between realtime and batch processing models. It offers aspects of batch processing; computation can be moved to the data via direct MapReduce support. It also supports realtime patterns with random access and fast</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/realtime-layer-batch-layer.png" title="Realtime and Batch Layers" ></p>

<p>reads and writes. So our realtime and batch layers can be viewed like this:</p>

<ol>
<li>Data entering the system is persisted in HBase.</li>
<li>MapReduce jobs are used to create artifacts useful to consumers at scale.</li>
<li>Incremental updates are handled in realtime by processing updates to HBase in a Storm cluster, and are applied to the artifacts produced by MapReduce jobs.</li>
</ol>


<h2>Processing HBase updates in realtime</h2>

<p>So new data lands in HBase but how does Storm know to process it? There is precedent here. Google’s <a href="http://research.google.com/pubs/pub36726.html">Percolator paper</a> describes a technique for doing so over BigTable: it writes a notification entry to a column family whenever a row changes. Processing components scan for notifications and process them as they enter the system.</p>

<p>This is the general approach we have taken to initiate processing in Storm. Google’s Percolator strategy does not translate directly to HBase. Differences in the way regions are managed versus BigTable tables made using a different column family impractical. So we use a separate &ldquo;notification&rdquo; table to track changes to the original.  Updates to HBase go through an API that writes notification entries as well as the data itself. We then wrote a specialized Storm spout that scans the notification table to initiate processing of updates.</p>

<p>The result is processing infrastructure like this, with Storm Spouts and bolts complementing conventional MapReduce processing:</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/processing-diagram.png" title="Processing Diagram" ></p>

<p>The processed data model may be another set of HBase tables, a relational database, or some other data store. Its design should be centered on the needs of the applications and services, letting the processing infrastructure build data for those needs. It is important to note that MapReduce output should be done with a bulk load operation in order to avoid saturating the processed data store with individual updates.</p>

<p>This basic model turns out to be robust. Volume spikes from source systems can be spread throughout the HBase cluster. There are a couple key steps for success here:</p>

<ul>
<li><p>Regular major compactions on the notification HBase tables are essential. Without major compactions, completed notifications will pile up and performance of the system will gradually degrade.</p></li>
<li><p>The notification tables themselves may be small in size, but should be aggressively split across the cluster. This spreads load to handle volume spikes and improve concurrency.</p></li>
</ul>


<p>Also note that MapReduce is still an important part of the system. It’s simply a better tool for batch operations like bringing a new data set online or re-processing an existing data set with new logic.</p>

<h2>Measure Everything</h2>

<p>There are a number of moving parts in this system, and good measurements are the best way to ensure it’s working well. For example, in development we found our HBase Region Servers would encounter frequent but short-lived process queues during heavy load. This didn’t look like an issue in HBase, but when we measured the performance of the calling process there was a noticeable degradation. The point is, instrumentation built into Hadoop and HBase are great but not sufficient. Measuring the observed performance at all layers is important to create an optimal system.</p>

<p>There are many good technologies for doing so. We generally use the <a href="https://github.com/codahale/metrics">Metrics API</a> by Coda Hale. Here is an example of HBase client throughput using an instrumented implementation of HTableInterface. The data is collected by the Metrics API and displayed with <a href="http://graphite.wikidot.com/">Graphite</a>:</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/measure-everything.png" title="Measure Everything" ></p>

<h2>Different models, same logic</h2>

<p>The same logic needs to be applied to both batch and stream processing despite the necessary differences in infrastructure. This is a challenge since the models speak very different languages: InputFormats describe an immutable and complete set of data, whereas event streams expose incremental changes without context.</p>

<p>It turns out the function is the only real commonality between them; simply taking a subset of input and returning useful output. So, our strategy is this:</p>

<p><em>Build all logic as a set of simple functions, then compose and coordinate those functions with higher-level processing libraries.</em></p>

<p>We use Storm to compose our realtime processing and Apache Crunch to compose our MapReduce jobs. Here are some lessons we have learned to apply this strategy effectively:</p>

<h4>Minimize intermediate state</h4>

<p>Persisting intermediate state can be expensive and creates complex relationships between moving parts. This is particularly true if a MapReduce job creates intermediate state used by realtime processing or vice versa. Instead, keep processing pipelines independent whenever possible and combine the results at the end.</p>

<h4>Isolate processing models</h4>

<p>Our MapReduce jobs are typically run on separate infrastructure than realtime processing to ensure expensive jobs do not saturate time-critical processing.</p>

<h4>Be aware of the semantic differences in the processing models</h4>

<p>A &ldquo;join&rdquo; in a MapReduce job sees all data, whereas a &ldquo;join&rdquo; in stream processing gets incremental subsets. If a function needs the full context to execute, that context must be externally loaded in the realtime processing system. In our case, external state is loaded from HBase and cached, but projects like <a href="http://engineering.twitter.com/2012/08/trident-high-level-abstraction-for.html">Trident</a> are now providing some aggregation facilities over storm as well.</p>

<h2>The path forward</h2>

<p>The patterns here have been successful but require significant scaffolding and infrastructure to bring together. Near-realtime processing demands over big data are bound to increase, which means there is an opportunity here; higher level abstractions should emerge. Similar to how tools like Crunch and Hive offer abstractions over MapReduce, it’s likely that similar primitives can express the patterns described here.</p>

<p>How these higher abstractions emerge remains to be seen, but there is one thing I’m sure of: it’s going to be fun.</p>

<h2>Acknowledgments</h2>

<p>I’d like to acknowledge key contributors to building this and related systems: Jason Bray, Ben Brown, Robert Farr, Preston Koprivica, Swarnim Kulkarni, Kyle McGovern, Andrew Olson, Mike Richards, Micah Whitacre, Greg Whitsitt, and others.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/evangelizing-user-experience/">Evangelizing User Experience</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/"><span class="fn">Cerner Engineering</span></a></span>
 | 








  


<time datetime="2013-02-12T00:00:00-06:00" pubdate data-updated="true">Feb 12<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>In the dark ages of development, great software meant packing in the functionality. People began doing more and more with their software. Updates meant newer and more exciting functionality. Sounds great, right? Of course it does, but something went horribly wrong. Slowly we became inundated with cluttered screens as software developers struggled to find a place to put their latest innovative functionality. Buttons began adding up and before we knew it, we were inventing user interface controls like ribbons to hold all the buttons.</p>

<p><em><strong>On the bright side, things can only get better from here.</strong></em>__</p>

<p><img class="center" src="/assets/2013-02-12-evangelizing-user-experience/roadNotes_before.png" title="RoadNotes" ></p>

<p>Somewhere along the straight and narrow path to UI nirvana, we strayed. Distracted by the allure of &ldquo;doing more,&rdquo; we forgot to question why more needed to be done in the first place. We overlooked the importance of designing how something should be done because we were busy discovering new things to do. And, most importantly, we failed to include the user in the development process. Instead, we used inaccurate perceptions and engineering constraints to dictate how users should interact with our solutions.</p>

<p>Fortunately, software developers everywhere are beginning to see the light.</p>

<p>We are entering a new golden age of software solutions—one where the greatness of software is not measured by the number of functions, but by its ease of use. Usability is in the spotlight more now than ever before. Perhaps you&rsquo;ve caught wind of some of these buzz words lately: Usability, User Experience, Interaction Design, Emotional Designer, and User-Centered Design. Generally speaking, they all point to the same thing: making software easier, more elegant, more intuitive, and (dare I say) more enjoyable to use.</p>

<p>Interaction designers may be the instigators of software development’s Great Awakening, but user experience experts cannot do the job alone. It takes a great team to bring everything together and produce an exceptional product. Without user-focused engineering, great ideas and concepts would never come to life, regardless of their theoretical merit. If leadership is not committed to doing whatever it takes to ensure the users have an intuitive, enjoyable experience with our solutions, entire projects and initiatives would never see the light of day. On the other hand, having a team of leaders, designers, and engineers working to produce software that users will love, can produce incredible results.</p>

<p>The creators of Paper by FiftyThree, Apple’s iOS App of the Year, understand the importance of working together to produce an awesome experience. The results of their work speak for themselves. Watch this short video to catch a glimpse of their development culture and ideals:</p>

<iframe width="560" height="315" class="aligncenter" frameborder="0" src="https://www.youtube-nocookie.com/embed/pCoIeqgQoJ4?rel=0"></iframe>


<p>We are about to make a big splash of our own in the world of iOS with the eminent release of a new iPad app for ambulatory physicians. Designing fast, smart, and easy workflows, and creating an elegantly robust and beautifully simple interface to go with them has helped foster the rapidly growing culture of flawless execution at Cerner. From the outset of the iOS initiative, user-centered design has been the main focus. We created simple and intuitive interactions for complex user processes. Then, when we thought we had it right, our team of user researchers ran our designs through extensive usability testing, which validated our concepts or helped us discover where we could improve. Based on their feedback, we tweaked the designs, and re-tested. We rinsed and repeated. We are confident that our clients will be pleased. Why? Because we’ve been talking to them throughout the entire process.</p>

<p><img class="center" src="/assets/2013-02-12-evangelizing-user-experience/pcTouch1.png" title="PowerChart Touch Ambulatory" ></p>

<p>For Paper by FiftyThree, success means getting their users in touch with their creative side. At Cerner, producing amazing software solutions ultimately means improving the workflows of clinicians across the world, which impacts the health and wellbeing of countless individuals. We have a mission, and that mission is to make our solutions intuitive enough so that they simply fade into the background, allowing clinicians to focus on what is truly important: their patients.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/why-engineering-health/">Why Engineering Health?</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/charlie-huggard"><span class="fn">Charlie Huggard</span></a></span>
 | 








  


<time datetime="2013-02-04T00:00:00-06:00" pubdate data-updated="true">Feb 4<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>Hello and welcome to a public face for Cerner Engineering, by Cerner Engineering associates, to talk about engineering, technology, and all of the other awesome things we do.</p>

<p>Cerner has been recognized as a visionary company, transforming the delivery of healthcare around the world. Improving the health of individuals and the delivery of care is an extremely large, complex, ever-changing problem. Along with the efforts of our strategists and consultant organizations, solving this problem takes a ton of smart folks in our Engineering and CernerWorks Hosting organizations, who are free to play with, adopt, and embrace new technologies and ways of working.</p>

<p>Speaking of working differently, I’m currently at a coffee shop, writing this introduction post on my Cerner-issued Mac. I’ve completed a few code reviews for my team online using <a href="http://www.atlassian.com/software/crucible">Crucible</a>, and have been pushing minor tweaks to the code behind this website to our <a href="https://enterprise.github.com/">GitHub Enterprise</a> instance. We collaborate on changes internally as well as directly with our clients using our <a href="http://www.jivesoftware.com/resources/customer-case-studies/cerner/">Jive-powered</a> <a href="https://connect.ucern.com">uCern Connect</a> site.</p>

<p>But it’s more than just how cool is my day&hellip; Cerner understands the value in participating in a meaningful dialog about technology, sharing our discoveries, and learning from our peers in the industry. That’s why this summer our Engineering organization will be holding it’s 3rd annual DevCon, where all of our Engineering associates take two days away from their normal work to present and share their ideas and discoveries with each other. Cerner holds regular Hack Nights, where pizza and drinks are provided to get people together to try out new ideas and new technologies and solve things that may or may not apply to our usual work. Cerner also sends associates to attend and present not only at conferences around technologies we already use, such as <a href="http://na.apachecon.com/">ApacheCon</a>, <a href="http://www.eclipsecon.org/">EclipseCon</a>, <a href="https://us.pycon.org">PyCon</a>, <a href="http://railsconf.com/">RailsConf</a> and <a href="https://developer.apple.com/wwdc/">WWDC</a>, but also at conferences around new technologies and ways of thinking like <a href="http://www.computemidwest.com/">Compute Midwest</a>, <a href="https://thestrangeloop.com/">Strange Loop</a> and <a href="http://sxsw.com/interactive">SXSW</a>.</p>

<p>In a similar vein, by launching this Engineering Health site, we hope to provide transparency to our internal organizations, as well create another way to connect to the broader community involved in solving big problems with software and technology. You will see that in addition to being a leader in the healthcare industry, Cerner is also at the forefront of software technologies. With our <a href="/2013/02/composable-mapreduce-with-hadoop-and-crunch/">inaugural blog post</a> from <a href="/engineers/ryan-brush/">Ryan Brush</a>, you will get a taste for some cool ways we’re putting big data processing methods to work for some of our new initiatives. Future blog posts will get into other technologies used, lessons learned, and just fun things direct and unfiltered from our Engineering organization.</p>

<p>I hope you enjoy this site, and feel free to connect with us on Twitter <a href="https://twitter.com/cernereng">@CernerEng</a>.</p>
</div>
  
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/blog/page/6/">&larr; Older posts</a></li>
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
    <li class="next"><a href="/blog/page/4/">Newer posts &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span4" role="complementary">
  
    <section class="well">
  <h2>Recent Posts</h2>
  <ul id="recent_posts" class="nav nav-list">
    
      <li class="post">
        <a href="/blog/scaling-people-with-apache-crunch/">Scaling People With Apache Crunch</a>
      </li>
    
      <li class="post">
        <a href="/blog/migrating-from-eclipse-3.x-to-eclipse-4.x-the-iaware-story/">Migrating From Eclipse 3.X to Eclipse 4.X - the iAware Story</a>
      </li>
    
      <li class="post">
        <a href="/2014/01/sponsoring-the-apache-software-foundation/">Sponsoring the Apache Software Foundation</a>
      </li>
    
      <li class="post">
        <a href="/2014/01/the-raft-protocol-a-better-paxos/">The Raft Protocol: A Better Paxos?</a>
      </li>
    
      <li class="post">
        <a href="/2014/01/cerner-and-open-source/">Cerner and Open Source</a>
      </li>
    
  </ul>
</section>

<section class="well">
  <h2>GitHub Repos</h2>
  <ul id="gh_repos" class="nav">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/cerner">@cerner</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'cerner',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer">
  <div class="container">
    <p>&copy; 2014 Cerner</p>
  </div>
</footer>

  










</body>
</html>
