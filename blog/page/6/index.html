
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Engineering Health</title>
  <meta name="author" content="Cerner">

  
  <meta name="description" content="">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://engineering.cerner.com/blog/page/6">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/site/site.css" media="screen, projection" rel="stylesheet" type="text/css">

  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <script src="/javascripts/libs/bootstrap.min.js"></script>

  <link href="/atom.xml" rel="alternate" title="Engineering Health" type="application/atom+xml">
  
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37701128-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body  class="top-navbar  ">
  <header id="banner" class="navbar navbar-fixed-top" role="banner">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">
        <img src="/images/logo2.png" title="Cerner Engineering Health">
      </a>

      <nav id="nav-main" class="nav-collapse" role="navigation">
  <ul id="menu-top-menu" class="nav pull-right">
    <li class="menu-life-as-an-engineer">
      <a href="/about">Life as an Engineer</a>
    </li>
    <li class="menu-who-we-are">
      <a href="/engineers">Who We Are</a>
    </li>
    <li>
      <a href="/blog/archives">Archives</a>
    </li>
  </ul>      
</nav>

    </div>
  </div>
</header>

<div class="container">
  <div class="row">
    <div class="span8 pull-left page-header">
      <img class="site-header__img" src="/images/skyline2.png" alt="">    
    </div>

    
      <div class="span4 pull-right"> 
        <div id="search-form-wrapper">
          <form class="form-search" id="searchform" role="search" action="https://google.com/search" method="get" >
            <label class="hide" for="q">Search for:</label>
            <input name="q" type="hidden" value="site:engineering.cerner.com" />
            <input class="search-query" id="q" name="q" type="text" results="0" placeholder="Search">
            <input class="btn" id="searchsubmit" type="submit" value="Search" >
          </form>      
        </div>
      </div>
    
  </div>
</div>

  
  <div class="container">
    <div class="row-fluid">
      <div id="main" class="span8" role="main">
  
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/near-real-time-processing-over-hadoop-and-hbase/">Near Real-time Processing Over Hadoop and HBase</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/ryan-brush"><span class="fn">Ryan Brush</span></a></span>
 | 








  


<time datetime="2013-02-27T00:00:00-06:00" pubdate data-updated="true">Feb 27<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><h2>From MapReduce to realtime</h2>

<p>This post covers much of the <a href="http://na.apachecon.com/schedule/presentation/161/">Near-Realtime Processing Over HBase</a> talk I’m giving at <a href="http://na.apachecon.com/">ApacheCon NA 2013</a> in blog form. It also draws from the <a href="http://strataconf.com/stratany2012/public/schedule/detail/25387">Hadoop, HBase, and Healthcare</a> talk from StrataConf/Hadoop World 2012.</p>

<p>The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics. For instance, if a user searches for &ldquo;heart disease&rdquo; and a patient has &ldquo;myocardial infarction&rdquo; documented, that document should be highly ranked in the results.</p>

<p>Analyzing and semantically annotating can be computationally expensive, especially when building indexes that could grow into the billions. Algorithms in this space may be discussed in a future blog post, but for now we focus on creation of an infrastructure up to the computational demands. For this, Hadoop is a great fit. A search index is logically a function of a set of input data, and MapReduce allows us to apply such functions in parallel across an arbitrarily large data set.</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/chart-search-screen.png" title="Chart Search" ></p>

<h4>A trend towards competing needs</h4>

<p>The above pattern is powerful but creates a nice problem to have: people want the output of the processing &mdash; in this case, updates to search indexes &mdash; faster. Since we cannot run a MapReduce job over our entire data set every millisecond, we encounter competing needs; the need to <em>process all data holistically</em> conflicts with the need to <em>quickly apply incremental updates</em> to that processing.</p>

<p>This difference may seem simple, but has deep implications.  For instance:</p>

<ul>
<li><p>With MapReduce we can move our computation to the data, but fast updates require moving data to computation.</p></li>
<li><p>MapReduce jobs produce output as a pure function of the input; realtime processing needs to handle outdated state. For instance, we build a phone book and a name changes from Smith to Jones, realtime processing must remove the outdated entry, whereas MapReduce simply rebuilds the whole phone book.</p></li>
<li><p>MapReduce jobs often assume a static set of complete data, whereas realtime processing may see partial data or new data introduce in an unexpected order.</p></li>
</ul>


<p>And despite these differences, our processing output must be the identical; we need to apply the same logic across very different processing models.</p>

<h2>Realtime and batch layers</h2>

<p>These significant differences mean different processing infrastructures. Nathan Marz described this well in his <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">How to Beat the CAP Theorem</a> post. The result is a system that uses complementary technologies: stream-based processing with <a href="http://storm-project.net/" title="">Storm</a> and batch processing with Hadoop.</p>

<p>Interestingly, <a href="http://hbase.apache.org/" title="">HBase</a> sits at a juncture between realtime and batch processing models. It offers aspects of batch processing; computation can be moved to the data via direct MapReduce support. It also supports realtime patterns with random access and fast</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/realtime-layer-batch-layer.png" title="Realtime and Batch Layers" ></p>

<p>reads and writes. So our realtime and batch layers can be viewed like this:</p>

<ol>
<li>Data entering the system is persisted in HBase.</li>
<li>MapReduce jobs are used to create artifacts useful to consumers at scale.</li>
<li>Incremental updates are handled in realtime by processing updates to HBase in a Storm cluster, and are applied to the artifacts produced by MapReduce jobs.</li>
</ol>


<h2>Processing HBase updates in realtime</h2>

<p>So new data lands in HBase but how does Storm know to process it? There is precedent here. Google’s <a href="http://research.google.com/pubs/pub36726.html">Percolator paper</a> describes a technique for doing so over BigTable: it writes a notification entry to a column family whenever a row changes. Processing components scan for notifications and process them as they enter the system.</p>

<p>This is the general approach we have taken to initiate processing in Storm. Google’s Percolator strategy does not translate directly to HBase. Differences in the way regions are managed versus BigTable tables made using a different column family impractical. So we use a separate &ldquo;notification&rdquo; table to track changes to the original.  Updates to HBase go through an API that writes notification entries as well as the data itself. We then wrote a specialized Storm spout that scans the notification table to initiate processing of updates.</p>

<p>The result is processing infrastructure like this, with Storm Spouts and bolts complementing conventional MapReduce processing:</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/processing-diagram.png" title="Processing Diagram" ></p>

<p>The processed data model may be another set of HBase tables, a relational database, or some other data store. Its design should be centered on the needs of the applications and services, letting the processing infrastructure build data for those needs. It is important to note that MapReduce output should be done with a bulk load operation in order to avoid saturating the processed data store with individual updates.</p>

<p>This basic model turns out to be robust. Volume spikes from source systems can be spread throughout the HBase cluster. There are a couple key steps for success here:</p>

<ul>
<li><p>Regular major compactions on the notification HBase tables are essential. Without major compactions, completed notifications will pile up and performance of the system will gradually degrade.</p></li>
<li><p>The notification tables themselves may be small in size, but should be aggressively split across the cluster. This spreads load to handle volume spikes and improve concurrency.</p></li>
</ul>


<p>Also note that MapReduce is still an important part of the system. It’s simply a better tool for batch operations like bringing a new data set online or re-processing an existing data set with new logic.</p>

<h2>Measure Everything</h2>

<p>There are a number of moving parts in this system, and good measurements are the best way to ensure it’s working well. For example, in development we found our HBase Region Servers would encounter frequent but short-lived process queues during heavy load. This didn’t look like an issue in HBase, but when we measured the performance of the calling process there was a noticeable degradation. The point is, instrumentation built into Hadoop and HBase are great but not sufficient. Measuring the observed performance at all layers is important to create an optimal system.</p>

<p>There are many good technologies for doing so. We generally use the <a href="https://github.com/codahale/metrics">Metrics API</a> by Coda Hale. Here is an example of HBase client throughput using an instrumented implementation of HTableInterface. The data is collected by the Metrics API and displayed with <a href="http://graphite.wikidot.com/">Graphite</a>:</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/measure-everything.png" title="Measure Everything" ></p>

<h2>Different models, same logic</h2>

<p>The same logic needs to be applied to both batch and stream processing despite the necessary differences in infrastructure. This is a challenge since the models speak very different languages: InputFormats describe an immutable and complete set of data, whereas event streams expose incremental changes without context.</p>

<p>It turns out the function is the only real commonality between them; simply taking a subset of input and returning useful output. So, our strategy is this:</p>

<p><em>Build all logic as a set of simple functions, then compose and coordinate those functions with higher-level processing libraries.</em></p>

<p>We use Storm to compose our realtime processing and Apache Crunch to compose our MapReduce jobs. Here are some lessons we have learned to apply this strategy effectively:</p>

<h4>Minimize intermediate state</h4>

<p>Persisting intermediate state can be expensive and creates complex relationships between moving parts. This is particularly true if a MapReduce job creates intermediate state used by realtime processing or vice versa. Instead, keep processing pipelines independent whenever possible and combine the results at the end.</p>

<h4>Isolate processing models</h4>

<p>Our MapReduce jobs are typically run on separate infrastructure than realtime processing to ensure expensive jobs do not saturate time-critical processing.</p>

<h4>Be aware of the semantic differences in the processing models</h4>

<p>A &ldquo;join&rdquo; in a MapReduce job sees all data, whereas a &ldquo;join&rdquo; in stream processing gets incremental subsets. If a function needs the full context to execute, that context must be externally loaded in the realtime processing system. In our case, external state is loaded from HBase and cached, but projects like <a href="http://engineering.twitter.com/2012/08/trident-high-level-abstraction-for.html">Trident</a> are now providing some aggregation facilities over storm as well.</p>

<h2>The path forward</h2>

<p>The patterns here have been successful but require significant scaffolding and infrastructure to bring together. Near-realtime processing demands over big data are bound to increase, which means there is an opportunity here; higher level abstractions should emerge. Similar to how tools like Crunch and Hive offer abstractions over MapReduce, it’s likely that similar primitives can express the patterns described here.</p>

<p>How these higher abstractions emerge remains to be seen, but there is one thing I’m sure of: it’s going to be fun.</p>

<h2>Acknowledgments</h2>

<p>I’d like to acknowledge key contributors to building this and related systems: Jason Bray, Ben Brown, Robert Farr, Preston Koprivica, Swarnim Kulkarni, Kyle McGovern, Andrew Olson, Mike Richards, Micah Whitacre, Greg Whitsitt, and others.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/evangelizing-user-experience/">Evangelizing User Experience</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/"><span class="fn">Cerner Engineering</span></a></span>
 | 








  


<time datetime="2013-02-12T00:00:00-06:00" pubdate data-updated="true">Feb 12<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>In the dark ages of development, great software meant packing in the functionality. People began doing more and more with their software. Updates meant newer and more exciting functionality. Sounds great, right? Of course it does, but something went horribly wrong. Slowly we became inundated with cluttered screens as software developers struggled to find a place to put their latest innovative functionality. Buttons began adding up and before we knew it, we were inventing user interface controls like ribbons to hold all the buttons.</p>

<p><em><strong>On the bright side, things can only get better from here.</strong></em>__</p>

<p><img class="center" src="/assets/2013-02-12-evangelizing-user-experience/roadNotes_before.png" title="RoadNotes" ></p>

<p>Somewhere along the straight and narrow path to UI nirvana, we strayed. Distracted by the allure of &ldquo;doing more,&rdquo; we forgot to question why more needed to be done in the first place. We overlooked the importance of designing how something should be done because we were busy discovering new things to do. And, most importantly, we failed to include the user in the development process. Instead, we used inaccurate perceptions and engineering constraints to dictate how users should interact with our solutions.</p>

<p>Fortunately, software developers everywhere are beginning to see the light.</p>

<p>We are entering a new golden age of software solutions—one where the greatness of software is not measured by the number of functions, but by its ease of use. Usability is in the spotlight more now than ever before. Perhaps you&rsquo;ve caught wind of some of these buzz words lately: Usability, User Experience, Interaction Design, Emotional Designer, and User-Centered Design. Generally speaking, they all point to the same thing: making software easier, more elegant, more intuitive, and (dare I say) more enjoyable to use.</p>

<p>Interaction designers may be the instigators of software development’s Great Awakening, but user experience experts cannot do the job alone. It takes a great team to bring everything together and produce an exceptional product. Without user-focused engineering, great ideas and concepts would never come to life, regardless of their theoretical merit. If leadership is not committed to doing whatever it takes to ensure the users have an intuitive, enjoyable experience with our solutions, entire projects and initiatives would never see the light of day. On the other hand, having a team of leaders, designers, and engineers working to produce software that users will love, can produce incredible results.</p>

<p>The creators of Paper by FiftyThree, Apple’s iOS App of the Year, understand the importance of working together to produce an awesome experience. The results of their work speak for themselves. Watch this short video to catch a glimpse of their development culture and ideals:</p>

<iframe width="560" height="315" class="aligncenter" frameborder="0" src="https://www.youtube-nocookie.com/embed/pCoIeqgQoJ4?rel=0"></iframe>


<p>We are about to make a big splash of our own in the world of iOS with the eminent release of a new iPad app for ambulatory physicians. Designing fast, smart, and easy workflows, and creating an elegantly robust and beautifully simple interface to go with them has helped foster the rapidly growing culture of flawless execution at Cerner. From the outset of the iOS initiative, user-centered design has been the main focus. We created simple and intuitive interactions for complex user processes. Then, when we thought we had it right, our team of user researchers ran our designs through extensive usability testing, which validated our concepts or helped us discover where we could improve. Based on their feedback, we tweaked the designs, and re-tested. We rinsed and repeated. We are confident that our clients will be pleased. Why? Because we’ve been talking to them throughout the entire process.</p>

<p><img class="center" src="/assets/2013-02-12-evangelizing-user-experience/pcTouch1.png" title="PowerChart Touch Ambulatory" ></p>

<p>For Paper by FiftyThree, success means getting their users in touch with their creative side. At Cerner, producing amazing software solutions ultimately means improving the workflows of clinicians across the world, which impacts the health and wellbeing of countless individuals. We have a mission, and that mission is to make our solutions intuitive enough so that they simply fade into the background, allowing clinicians to focus on what is truly important: their patients.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/why-engineering-health/">Why Engineering Health?</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/charlie-huggard"><span class="fn">Charlie Huggard</span></a></span>
 | 








  


<time datetime="2013-02-04T00:00:00-06:00" pubdate data-updated="true">Feb 4<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>Hello and welcome to a public face for Cerner Engineering, by Cerner Engineering associates, to talk about engineering, technology, and all of the other awesome things we do.</p>

<p>Cerner has been recognized as a visionary company, transforming the delivery of healthcare around the world. Improving the health of individuals and the delivery of care is an extremely large, complex, ever-changing problem. Along with the efforts of our strategists and consultant organizations, solving this problem takes a ton of smart folks in our Engineering and CernerWorks Hosting organizations, who are free to play with, adopt, and embrace new technologies and ways of working.</p>

<p>Speaking of working differently, I’m currently at a coffee shop, writing this introduction post on my Cerner-issued Mac. I’ve completed a few code reviews for my team online using <a href="http://www.atlassian.com/software/crucible">Crucible</a>, and have been pushing minor tweaks to the code behind this website to our <a href="https://enterprise.github.com/">GitHub Enterprise</a> instance. We collaborate on changes internally as well as directly with our clients using our <a href="http://www.jivesoftware.com/resources/customer-case-studies/cerner/">Jive-powered</a> <a href="https://connect.ucern.com">uCern Connect</a> site.</p>

<p>But it’s more than just how cool is my day&hellip; Cerner understands the value in participating in a meaningful dialog about technology, sharing our discoveries, and learning from our peers in the industry. That’s why this summer our Engineering organization will be holding it’s 3rd annual DevCon, where all of our Engineering associates take two days away from their normal work to present and share their ideas and discoveries with each other. Cerner holds regular Hack Nights, where pizza and drinks are provided to get people together to try out new ideas and new technologies and solve things that may or may not apply to our usual work. Cerner also sends associates to attend and present not only at conferences around technologies we already use, such as <a href="http://na.apachecon.com/">ApacheCon</a>, <a href="http://www.eclipsecon.org/">EclipseCon</a>, <a href="https://us.pycon.org">PyCon</a>, <a href="http://railsconf.com/">RailsConf</a> and <a href="https://developer.apple.com/wwdc/">WWDC</a>, but also at conferences around new technologies and ways of thinking like <a href="http://www.computemidwest.com/">Compute Midwest</a>, <a href="https://thestrangeloop.com/">Strange Loop</a> and <a href="http://sxsw.com/interactive">SXSW</a>.</p>

<p>In a similar vein, by launching this Engineering Health site, we hope to provide transparency to our internal organizations, as well create another way to connect to the broader community involved in solving big problems with software and technology. You will see that in addition to being a leader in the healthcare industry, Cerner is also at the forefront of software technologies. With our <a href="/2013/02/composable-mapreduce-with-hadoop-and-crunch/">inaugural blog post</a> from <a href="/engineers/ryan-brush/">Ryan Brush</a>, you will get a taste for some cool ways we’re putting big data processing methods to work for some of our new initiatives. Future blog posts will get into other technologies used, lessons learned, and just fun things direct and unfiltered from our Engineering organization.</p>

<p>I hope you enjoy this site, and feel free to connect with us on Twitter <a href="https://twitter.com/cernereng">@CernerEng</a>.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/composable-mapreduce-with-hadoop-and-crunch/">Composable MapReduce With Hadoop and Crunch</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  

<span class="byline author vcard">Written by <a href="/engineers/ryan-brush"><span class="fn">Ryan Brush</span></a></span>
 | 








  


<time datetime="2013-02-03T00:00:00-06:00" pubdate data-updated="true">Feb 3<span>rd</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>Most developers know this pattern well: we design a set of schemas to represent our data, and then work with that data via a query language. This works great in most cases, but becomes a challenge as data sets grow to an arbitrary size and complexity. Data sets can become too large to query and update with conventional means.</p>

<p>These challenges often arise with Hadoop, simply because Hadoop is a popular tool to tackle such data sets. It&rsquo;s tempting to apply our familiar patterns: design a data model in Hadoop and query it. Unfortunately, this breaks down for a couple of reasons:</p>

<ol>
<li>For a large and complex data set, no single data model can efficiently handle all queries against it.</li>
<li>Even if such a model could be built, it would likely get bogged down in its own complexity and competing needs of different queries.</li>
</ol>


<p>So how do we approach this? Let&rsquo;s look to an aphorism familiar to long-term users of Hadoop:</p>

<h2 align="center">
<em>
Start with the questions to be answered, then model the data to answer them.
</em>
</h2>


<p>Related sets of applications and services tend to ask related questions. Applications doing interactive search queries against a medical record can use one data model, but detecting candidates for health management programs may need another. Both cases must have completely faithful representations of the original data.</p>

<p>Another challenge is leveraging common processing logic between these representations: there may be initial steps of data normalization and cleaning that are common to all needs, and other steps that are useful for some cases. One strategy is for each shared piece of logic to write output to its own data store, which can then be picked up by another job. Oversimplifying, it may look like this:</p>

<p><img class="center" src="/assets/2013-02-03-composable-mapreduce-with-hadoop-and-crunch/diagram1.png" title="Diagram 1" ></p>

<p>Such a model can be coordinated with Hadoop-based tools like <a href="http://oozie.apache.org/">Oozie</a>. But this model of persisting every processing stage and using them downstream has some drawbacks:</p>

<ol>
<li>The structure and location of each intermediate state must be externally defined, creating a barrier to easily leverage the output of one operation in another.</li>
<li>Data is duplicated unnecessarily. If one of our processing steps requires intermediate data, we must persist it, even if it is used only by other processing steps. Ideally, we could just connect the processing steps without unnecessary persistence. (In some cases, Hadoop itself persists intermediate state for MapReduce jobs, but there are many useful jobs that don&rsquo;t need to do so.)</li>
<li>Each step must be fully processed before the next step can run. Therefore, each processing step is only as fast as its slowest component.</li>
<li>Each persistent state must use a data model that can be processed efficiently in bulk. This limits our options for the data store. We must choose something MapReduce friendly, rather than optimizing for the needs of application queries.</li>
</ol>


<p>So how do we solve this? Rather than making intermediate data stores as the point of reuse, let&rsquo;s reason about the system at a higher level: make abstract, distributed data collections our point of reuse for processing. A data collection is a set of data that can be persisted to an arbitrary store when it makes sense, or streamed between processing steps when no persistence is needed. One data collection can be converted to another by applying functions to it. So the above diagram may now look like this, where arrows are functions used to transform data collections:</p>

<p><img class="center" src="/assets/2013-02-03-composable-mapreduce-with-hadoop-and-crunch/diagram2.png" title="Diagram 2" ></p>

<p>This has several advantages:</p>

<ol>
<li>Processing logic becomes reusable. By writing functions that consume and produce collections of data, we can efficiently share processing code as needed.</li>
<li>Data no longer needs to be stored for processing purposes. We can choose to store it when there is some other benefit.</li>
<li>The data models need not account for downstream processing; they can align to the needs of apps and services.</li>
<li>Processing is significantly more efficient as it can stream from one collection to another.</li>
</ol>


<p>This model supports storing and launching processing from intermediate states, but it doesn&rsquo;t require it. Processing downstream items from a raw data set will probably be a regular occurrence, but that need not be the case for other collections.</p>

<p>Perhaps the biggest advantage of this approach is that it makes MapReduce pipelines composable. Logic expressed as functions can be reused and chained together as necessary to solve a problem at hand. Any intermediate state can optionally be persisted, either as a cache of items expensive to process or an artifact useful to applications.</p>

<h4>Implementation Strategy</h4>

<p>So, how does this work?  Here are the key pieces:</p>

<ul>
<li>All processing input comes from a source. For Cerner, the source is typically data stored in Hadoop or HBase, but other implementations are possible.</li>
<li>Each source is converted into a data collection, which is described above.</li>
<li>One or more functions can be run against each data collection, converting it from one form to another. We&rsquo;ll discuss below how this be very efficient.</li>
<li>Collections can be persisted at any point in the processing pipeline. The persisted collections could be used for external tooling, or simply as a means to cache the results of an expensive computation.</li>
</ul>


<p>Fortunately, a newer MapReduce framework supports this pattern well: <a href="http://incubator.apache.org/crunch/">Apache Crunch</a>, based on Google&rsquo;s <a href="http://dl.acm.org/citation.cfm?id=1806638">FlumeJava paper</a>, represents each data set as a sort of distributed collection, and allows them to be composed together with strongly-typed functions. The output of a Crunch pipeline may be a data model easily loaded into a RDBMs system, inverted index or queried via tools like Apache Hive.</p>

<p>Crunch will also fuse steps in the processing pipeline whenever possible.  This means we can chain our functions together, and they&rsquo;ll automatically be run in the same process. This optimization is significant for many classes of jobs.  And although persisting intermediate state must be done by hand today, it will <a href="https://issues.apache.org/jira/browse/CRUNCH-145">likely be coming</a> in a future version of Crunch itself.</p>

<h3>An end to direct MapReduce jobs</h3>

<p>We may have reached the end of direct implementation of MapReduce jobs. Tools like Cascading and Apache Crunch offer excellent higher-level libraries, and Domain-Specific Languages like Hive and Pig allow the simple creation of queries or processing logic. Here at Cerner, we tend to use Crunch for pipeline processing and Hive for ad hoc queries of data in Hadoop.</p>

<p>MapReduce is a powerful tool, but it may be best viewed as a building block. Composing functions across distributed collections that use MapReduce as its basis lets us reason and leverage our processing logic more effectively.</p>
</div>
  
  


    </article>
  
  <ul class="pager">
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
    <li class="next"><a href="/blog/page/5/">Newer posts &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span4" role="complementary">
  
    <section class="well">
  <h2>Recent Posts</h2>
  <ul id="recent_posts" class="nav nav-list">
    
      <li class="post">
        <a href="/blog/intern-hackfest-2014/">Intern HackFest 2014</a>
      </li>
    
      <li class="post">
        <a href="/blog/the-plain-text-is-a-lie/">The Plain Text Is a Lie</a>
      </li>
    
      <li class="post">
        <a href="/blog/shipit-hackathon-mplus/">ShipIt - 24-hour Hackathon for Millennium+ Platform Dev</a>
      </li>
    
      <li class="post">
        <a href="/blog/scaling-people-with-apache-crunch/">Scaling People With Apache Crunch</a>
      </li>
    
      <li class="post">
        <a href="/blog/migrating-from-eclipse-3.x-to-eclipse-4.x-the-iaware-story/">Migrating From Eclipse 3.X to Eclipse 4.X - the iAware Story</a>
      </li>
    
  </ul>
</section>

<section class="well">
  <h2>GitHub Repos</h2>
  <ul id="gh_repos" class="nav">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/cerner">@cerner</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'cerner',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer">
  <div class="container">
    <p>&copy; 2014 Cerner</p>
  </div>
</footer>

  










</body>
</html>
