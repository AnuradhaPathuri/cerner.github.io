
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Engineering Health</title>
  <meta name="author" content="Cerner">

  
  <meta name="description" content="">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://engineering.cerner.com/blog/page/6">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/bootstrap/bootstrap.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/bootstrap/responsive.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/syntax/syntax.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/site/site.css" media="screen, projection" rel="stylesheet" type="text/css">

  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <script src="/javascripts/libs/bootstrap.min.js"></script>

  <link href="/atom.xml" rel="alternate" title="Engineering Health" type="application/atom+xml">
  
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37701128-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body  class="top-navbar  ">
  <header id="banner" class="navbar navbar-fixed-top" role="banner">
  <div class="navbar-inner">
    <div class="container">
      <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </a>

      <a class="brand" href="/">
        <img src="/images/logo2.png" title="Cerner Engineering Health">
      </a>

      <nav id="nav-main" class="nav-collapse" role="navigation">
  <ul id="menu-top-menu" class="nav pull-right">
    <li class="menu-life-as-an-engineer">
      <a href="/about">Life as an Engineer</a>
    </li>
    <li class="menu-who-we-are">
      <a href="/engineers">Who We Are</a>
    </li>
    <li>
      <a href="/blog/archives">Archives</a>
    </li>
  </ul>      
</nav>

    </div>
  </div>
</header>

<div class="container">
  <div class="row">
    <div class="span8 pull-left page-header">
      <img class="site-header__img" src="/images/skyline2.png" alt="">    
    </div>

    
      <div class="span4 pull-right"> 
        <div id="search-form-wrapper">
          <form class="form-search" id="searchform" role="search" action="https://google.com/search" method="get" >
            <label class="hide" for="q">Search for:</label>
            <input name="sitesearch" type="hidden" value="engineering.cerner.com" />
            <input class="search-query" id="q" name="q" type="text" results="0" placeholder="Search">
            <input class="btn" id="searchsubmit" type="submit" value="Search" >
          </form>      
        </div>
      </div>
    
  </div>
</div>

  
  <div class="container">
    <div class="row-fluid">
      <div id="main" class="span8" role="main">
  
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/04/ruby-at-cerner/">Ruby at Cerner</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  


  <span class="byline author vcard">Written by <span class="fn">Cerner Engineering</span></a></span>

 | 








  


<time datetime="2013-04-05T00:00:00-05:00" pubdate data-updated="true">Apr 5<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>Cerner&rsquo;s journey with Ruby started in the summer of 2009. We selected Ruby on Rails for rapid development of the <a href="https://store.cerner.com/">Cerner Store</a> to prepare for release at the Cerner Health Conference that year. In three months, with three engineers and a designer, we wrote and released the first version of an e-commerce web application. Two of those engineers, including me, had never worked with Ruby before but quickly realized the power and expressiveness of the language due to resources like <a href="http://mislav.uniqpath.com/poignant-guide/">Why&rsquo;s (Poignant) Guide to Ruby</a>.</p>

<p><img class="center" src="/assets/2013-04-05-ruby-at-cerner/the.foxes-4c.png" title="Chunky bacon!" ></p>

<p>Our experience with the Cerner Store taught us that Rails led to high productivity. Ruby is a very natural language to write code in and principles like <a href="http://en.wikipedia.org/wiki/Convention_over_configuration">convention over configuration</a> enabled us to solve our problems instead of spending time wrangling the framework. In addition, we valued the good practices of the Ruby community like easy-to-understand code and thorough unit testing with tools that aren&rsquo;t painful.</p>

<p>In the summer of 2010, we attended the first <a href="http://rubymidwest.com/">Ruby Midwest</a> as a team. We learned about developments in Ruby like JRuby and Chef as well as some of the great gems under development. The Cerner Store continued to grow and we learned about maintaining a Rails web app over time.</p>

<p>In early 2012, we were planning a massive undertaking to create a new platform for our Clients&#8217; healthcare data. It was to be called Millennium+ and it needed an architecture that could scale well to petabytes of data and dozens of engineers across many teams. We planned a <a href="http://engineering.cerner.com/2013/02/near-real-time-processing-over-hadoop-and-hbase/">service-oriented architecture</a> and chose Rails to serve as the server side of the application. Our Rails services call JVM services that retrieve data from HBase and serves the resulting data as JSON to the client-side applications, including our iOS app, PowerChart Touch Ambulatory. The high productivity we enjoyed on a small team scaled well to a large team of people who had never written Ruby before.</p>

<p>This was the start of Cerner&rsquo;s Ruby community. We developed reusable libraries and development processes that we continue to use today. The complexities of our architecture also led to the adoption of <a href="http://www.opscode.com/chef/">Chef</a> to automate operations and build a devops mindset. Chef is another integral use of Ruby that penetrates teams that did not use Ruby at all.</p>

<p>When planning our newest platform, <a href="http://www.cerner.com/blog/population_health_management_collaboration_with_advocate_physician_partners/">Population Health</a>, we needed to determine which platform made the most sense for a large-scale development of web applications on a tight timeline. We decided on Rails due in large part to its prioritization of convention over configuration, as well as the existing Ruby community at Cerner. These attributes would enable us to develop applications at the planned pace and scale. This decision is now disseminated to dozens of engineers working on brand new web applications and REST services powering them.</p>

<p>We&rsquo;ve bolstered our internal Ruby community with lots of documentation and guidelines. We make use of the <a href="http://ruby.railstutorial.org/ruby-on-rails-tutorial-book">Ruby on Rails Tutorial book</a> and Why&rsquo;s (Poignant) Guide to train engineers in Ruby. Bozhidar Batsov&rsquo;s <a href="https://github.com/bbatsov/ruby-style-guide">Ruby style guide</a> aligns closely with how we write our code, so we use <a href="https://github.com/bbatsov/rubocop">rubocop</a> to keep ourselves in line. Our development devices run on <a href="https://rvm.io/">RVM</a> and our servers run on <a href="https://www.phusionpassenger.com/">Phusion Passenger</a>.</p>

<p>Ruby has quickly become a very important part of Cerner&rsquo;s engineering culture, primarily as the language behind Rails and Chef. An internal Ruby meetup has begun and there have been presentations involving Ruby at our annual Developer Conference.</p>

<p><img class="center" src="/assets/2013-04-05-ruby-at-cerner/ruby-midwest-logo-2013.png" title="Ruby Midwest 2013 Logo" ></p>

<p>Additionally, we are sponsoring <a href="http://rubymidwest.com/">Ruby Midwest</a> because we want developers to know that Ruby is highly valued at Cerner. We&rsquo;re also sending a large number of our own associates there to learn.</p>

<p>We look forward to bringing on more engineers to use Ruby and other technologies to engineer the future of healthcare.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/03/modularity-in-medical-imaging/">Modularity in Medical Imaging</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  


  <span class="byline author vcard">Written by <span class="fn">Cerner Engineering</span></a></span>

 | 








  


<time datetime="2013-03-29T00:00:00-05:00" pubdate data-updated="true">Mar 29<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><p>Developers often take for granted the level of flexibility and customization that is available within the software they use every day. Consumers of imaging software have traditionally been completely confined to interpret exams a specific way, and frequently in ways that are unintuitive. Every physician, specialist, technologist, med student, and others across the continuum of care has a preference as to not only how exams are laid out, but what information is displayed with the images and what transformations would be applied or processed automatically. As it turns out, the only workflow they have in common is that they all view exams differently and they all want the experience to be clear and understandable.</p>

<p>We decided to take this to heart in the development of our viewing solution by allowing the application react to the user, opposed to the user reacting to the application and constraining their workflow. Working in the medical community can already be a high stress environment; introducing a piece of software intended to augment patient care that often times makes delivering care more difficult is not a viable solution. Finding the best way to address these issues starts with knowing the solution’s target audience. Before developers can help streamline a workflow, they first have to understand it. If you understand the user’s workflow and how they expect the solution to function, you can design for usability and they will want to use it, as opposed to feeling forced to use it. A key aspect of this is the reliability and simplicity of the application. If the user feels like the application is unstable or overly complex, they will immediately lose interest and view the whole experience as a chore, as opposed to something that can provide value.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-1.png" title="Figure 1" ></p>

<p>The interface is the first application element the user encounters. If it feels intimidating and complex, before they even perform any functions, you have set a negative tone for your application and adoption will be low. In the case of imaging, Radiologists often spend more than 60 hours a week reading over 16,000 procedures per year, all in front of the same application. Imagine working with an application that causes frustration throughout your day.  The first impression can propel an application in the marketplace or stifle it.</p>

<p>With the advent of open source software coming to the forefront, there are a variety of options for developing rich applications backed by a large community of contributors. One of those happens to be the Eclipse Rich Client Platform (RCP) and the corresponding Standard Widget Toolkit (SWT). RCP and SWT both provide a way for developers to rapidly develop a professional looking and stable application but are somewhat limiting when viewed from an imaging standpoint. Radiologists typically read on workstations that have three or more monitors and in environments that are dimly lit. Using an application that is primarily grey and white can not only cause eye strain for the user but it can be distracting from what the user is trying to accomplish.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-2.png" title="Figure 2" ></p>

<p>Our team addressed this by starting out with the native components and then closely working with radiologists and visual designers to plan a user interface that was not only visually pleasing but also functional. The end result was a set of skinnable SWT widgets that are reusable within any SWT based application and can be specifically themed to match a desired color scheme for an environment.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-3.png" title="Figure 3" >)</p>

<p>Another aspect of imaging that we needed to plan for was the wide array of uses (diagnostic versus distribution). In a diagnostic scenario, radiologists will use the solution to perform diagnosis and will use the application within a multi-monitor high-resolution environment. For purposes of distribution imaging, clinicians will view images for reference and often times will view these on a much lower resolution single screen device (such as a laptop). While these two scenarios have vastly different hardware environments, users want the same experience across both and do not want to have to learn a new application depending on the workflow they are targeting at the time. As developers, we can address this by using service APIs instead of hard implementations and then create different assemblies that are easily accessible based on what the user is trying to accomplish. This not only helps address production concerns but allows developers to code against mock data sources and stores using the same API that they would within a fully scaled production environment.</p>

<p>Once we had the basics covered, we rethought how we approached the workflow, and sought to intentionally design for usability and responsiveness within the application. Users previously were constrained to grids and a limited amount of information while reading. With modular components designed to perform specific tasks and provide specific information, the user can now decide how they want to consume and view exams.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-4.png" title="Figure 4" ></p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-5.png" title="Figure 5" ></p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-6.png" title="Figure 6" ></p>

<p>So now we have a solid application for use within diagnostic scenarios, but how do we give access to the enterprise without adding overhead? Following the path of reuse, we decided that we would use the main application that we already had. By starting with these components instead of rebuilding it for the browser or an entirely new framework, we could focus on how we could change the delivery of that application.</p>

<p>We needed a way to deliver the same high power application without requiring the high power hardware. Traditional IT solutions would use Citrix, VMWare, or some other type of commercially available virtualization solution. While these are all great solutions for certain scenarios, the requirement of plugin installation on the client device accessing the application the heavy graphics processing within an imaging solution does not play nicely in these environments.  We can build on what these other solutions use (MS Remote Desktop Services) but choose to marshal the end result in an entirely different way.</p>

<p>If we want to deliver an application via a web browser, we have a limited selection of tools that we can depend on being available. IE has its own quirks and implementations, Chrome has some fancy extras on top of the WebKit rendering engine, and while Safari/Firefox use the same engine they still behave differently. Developing and deploying an application that must be focused on performance for the end user and still usable within a variety of environments has to take all of these constraints and challenges into consideration.</p>

<p>First, we needed a way to communicate between the client and server. The HTML5 WebSocket API is great for this, but only has minimal support and user adoption. Socket.IO has a great framework that allows for WebSocket emulation (or their native use if available) that provides great performance. We decided to use this for our real-time web app and it has proven to be invaluable throughout our development process.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-7.png" title="Figure 7" ></p>

<p>Now that we can communicate to the server what the user wants to do, we need a way to provide a representation of the images and UI that corresponds to the user input. The HTML5 Canvas API would seem like a good choice but continually redrawing canvas areas is process intensive and as this API isn’t supported within IE (the vast majority of our user base), it simply wasn’t an option. Instead, we decided to use a streaming mechanism, but couldn’t use the HTTP Streaming spec as this inherently introduces latency, which wouldn’t provide an acceptable user experience. All browsers support JPEG and PNG decompression and we can easily compress representations of the screen and shuttle these to the client. Even better, we can do this with a minimal amount of latency (we eliminate the need for key frames within an MPEG stream); we can choose which frames to drop and we can adjust the quality of each frame on the fly. This allows the client to adapt to the current network constraints and machine capabilities by tracking the effective FPS and adjusting the parameters on a sliding scale accordingly.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-8.png" title="Figure 8" ></p>

<p>With all of these tiers in place, we now have an application in the browser that looks identical to the desktop deployment with plugins or installation required.</p>

<p><img class="center" src="/assets/2013-03-29-modularity-in-medical-imaging/Figure-9.png" title="Figure 9" ></p>

<p>Beyond that, we have an SDK where third parties can build on top of our current deployment and provide even more specialized functionality within the imaging space, while still allowing users to customize their layout and how the data is delivered. We have now managed to deliver the same application to any device with a browser and our developers can develop from a single code line. This pleases our users and makes our developers’ lives easier. A true testament to how ease of development and a simple, yet powerful, user experience can exist in the same application.</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/03/cerner-and-tycho/">Cerner and Tycho</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  


  <span class="byline author vcard">Written by <a href="/engineers/jonny-wright"><span class="fn">Jonny Wright</span></a></span>

 | 








  


<time datetime="2013-03-27T00:00:00-05:00" pubdate data-updated="true">Mar 27<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><h2>Introduction</h2>

<p><a href="http://eclipse.org/tycho/">Tycho</a> is a build tool for integration between <a href="http://www.eclipse.org/pde/">PDE</a> and <a href="http://maven.apache.org/">Maven</a>.  Cerner has a long history of working with <a href="http://www.eclipse.org/home/categories/rcp.php">Eclipse RCP</a> but <a href="http://www.eclipse.org/equinox/p2/">P2</a> integration is something of a more recent phenomenon.  This post is going to talk about how Cerner is using Tycho, what prompted our transition to Tycho and how we accomplished moving our build to using Tycho.</p>

<h2>Why we use Tycho</h2>

<p>The first question when evaluating any tool is asking yourself: &ldquo;why would I use that&rdquo;?  In our case, Cerner already has an extensive Maven ecosystem.  For years it has been common practice, when developing Java, to use Maven as the go to build, site, and deployment tool.  What we needed was a set of extensions that allowed us to use all the facets of Maven while creating PDE artifacts.  Traditionally, the set of PDE artifacts that we needed to create and manage was limited to products, targets, and bundles.  Using the Maven conventions, we could build out all of these PDE artifacts while allowing the Maven POM to be the single source of truth for most build projects.  This worked well enough until we needed to rework our assemblies to be comprised of features, inserting another PDE artifact into the build lifecycle.</p>

<p>When we first began RCP development, no tooling existed to help us with creating PDE artifacts through Maven, so we created our own.  When the time came to start integrating additional PDE concepts, so that we could begin the path of integrating with P2 director, there was Tycho.</p>

<h2>How we use Tycho</h2>

<p>We use tycho generally to build the following set of PDE artifacts:</p>

<ul>
<li>Products</li>
<li>Features</li>
<li>Mirrors + Targets</li>
<li>Mirrors</li>
</ul>


<p>While the mirrors are not technically Eclipse artifacts, we use Tycho to mirror the Eclipse (for now Juno) repositories so that our builds are not hammering the Eclipse update site.  When you have 10&rsquo;s to 100&rsquo;s of users updating targets and running builds, it is just common courtesy to have your own mirror.</p>

<h3>Features</h3>

<p>We began our journey with Tycho by using it to generate P2 repositories from feature definitions.  The bulk of the heavy lifting is accomplished through the use of the tycho-packaging-plugin (as well as the base plugin, tycho-maven-plugin).  Since we deploy all of our plugins through maven, it is ideal to have Maven (Tycho in this case) do the dependency resolution for us and fill out the contents of our feature.  It also does all the work of turning that feature into a p2 repository.</p>

<p>This was actually a nontrivial exercise for us.  While we could have used a single feature definition, we tightly controll the dependencies that are pulled into our assembly.  Therefore, as we looked to breakdown our feature definition it was more complicated than simply pulling in the Juno feature and heaping everything into a feature on top of that.  Since we only use a subset of the Eclipse platform, we only want to pull forward the pieces we use explicitly (we also ran into some rather strange behavior with the Eclipse Jetty features).  We wanted to define the features in such a way that consumers of our platform could select a single feature that represented our core platform + Eclipse (as well as layer in additional functionality related to building their application).  In the end, we defined a set of about 20 features that rolled into a single working repository.  This way, consumers only needed to know the location of a single repository and then could use the metadata in the features to determine which features they needed.</p>

<h3>Products</h3>

<p>Starting at features made sense because there was no existing infrastructure for us to overhaul to build out the features.  The products were a different story.  With the products, we had an entire set of plugins that managed which version of Eclipse was pulled in for the product, how the .product was generated, and how we rolled that into an .exe.</p>

<p>The extra time we took in order to define the features thoroughly actually made this process significantly easier.  Today, we build a set of 8 applications:</p>

<ul>
<li>CareAware Criticalcare Dashboard</li>
<li>CareAware Criticalcare Personalized</li>
<li>CareAware Infusion Management Dashboard</li>
<li>CareAware Infusion Management Personalized</li>
</ul>


<p>Each of these products also has a corresponding mock build (where we substitute in mock resources to populate data for our views).  The definitions of these products was actually very simple and each ended up being around 8-10 features to create the product.  With the aggregated repository, we had a single repository in each POM where the versions for the features were determined and filled out.  Using the tycho-packaging-plugin and the tycho-p2-director plugin, we were able to automate the build of the corresponding p2 repositories which resulted from the Tycho builds.</p>

<h3>Mirrors + Targets</h3>

<p>Being the kind citizens that we are, we didn&rsquo;t want to abuse the Eclipse p2 release repository (also we didn&rsquo;t want them to cut us off like Maven Central).  To be good citizens in this build world, we decided we needed to mirror the Eclipse repositories ourselves.  Luckily for us, Tycho developers had already anticipated that need!  By using the <a href="http://wiki.eclipse.org/Tycho/Additional_Tools">tycho-p2-extras-plugin</a>, we were able to easily automate the mirroring of the required features from the Eclipse release repository.  Score (another) 1 for Tycho.</p>

<p>As part of the process for deploying these mirrors, we actually used the build-helper-plugin to deploy .target files to point to the mirrors into our maven repository.</p>

<h3>Why not bundles?</h3>

<p>The all important question: Why didn&rsquo;t we do bundles?  The two biggest things stopping us from uplifting our bundles was the lack of an easy way to incorporate non bundle sources into builds and the difficulty in generating sites with code coverage for projects that use both Java and Groovy (a sad state of affairs I don&rsquo;t recommend).  In the end, these hurdles plus the amount of time it would take to uplift our own projects to match the expected build structure was a significant overhaul.  We decided that, for now, the maven-bnd-plugin and m2eclipse can support our needs well enough for bundle development.</p>

<h2>From Custom Maven To Tycho</h2>

<p>So I&rsquo;ve mentioned throughout this blog post about how we moved from custom maven to tycho; so what does that mean exactly?  Back at the dawn of time (as I reckon it, others may know it as 2007), Cerner set down the path of doing Eclipse RCP development.  At the time, there weren&rsquo;t many good tools to automate your PDE builds.  Until very recently, most of the Eclipse projects have used Ant as their build system.  For our ecosystem and needs, it was Maven or bust.  So we set about creating plugins that would help us at all levels of the build.  Maven plugins were created for all of the following actions:</p>

<ul>
<li>Creating prodcuts</li>
<li>Creating Manifests for bundles</li>
<li>Running OSGi tests</li>
<li>Generating OSGi diff reports (on the manifests)</li>
<li>Accessing mirrored Eclipse instances</li>
<li>Generating target platforms based on POM configuration</li>
</ul>


<p>This turned out to be a rather extensive amount of work.  The model worked relatively successfully until a consumer came along with a set of requirements that read an awful lot like: &ldquo;we&rsquo;d like P2 integration included if we move to the iAware platform&rdquo;.  This threw a wrench in our system, because while P2 can work (and does) work by simply using .product files to define content sets, we had no tooling to generate P2 repositories.  We also had no mechanisms that would allow us to update only parts of their application (since this was 5 development teams working in tandem to produce a single assembly).  When all of this converged, we realized our build system had become insufficient and it was time to explore alternatives.  As you can see from the path we took, we started with the least intrusive changes and scaled up from there.  As of this blog, iAware is preparing for the first release using features and products.</p>

<h2>How do we deploy?</h2>

<p>Get ready for one ugly POM configuration:</p>

<p><img class="center" src="/assets/2013-03-27-cerner-and-tycho/P2-PomConfiguration-Edit2.png" title="P2 POM Configuration" ></p>

<p>If that isn&rsquo;t enough to scare you, I don&rsquo;t know what is.  I wrote it and it terrifies me.  What it really means though is that I haven&rsquo;t found a great way for deploying p2 repositories as part of the maven build process.  Options exist, such as deploying the zip file of the repository into the maven repo or writing shell scripts to automate the deployment, but neither of these sat well with me.  So I went for the least distasteful choice and wrote some basic ant scripts :).</p>

<p>All in all, Tycho has become a large part of our development ecosystem.  This blog only really touches the surface, but Tycho has become integral at all stages (besides bundle dev) to managing our assemblies.  So, thank you Tycho contributors!</p>
</div>
  
  


    </article>
  
  
    <article class="entry">
      
  <header class="entry-header">
    
      <h1 class="entry-title"><a href="/2013/02/near-real-time-processing-over-hadoop-and-hbase/">Near Real-time Processing Over Hadoop and HBase</a></h1>
    
    
      <div class="entry-meta meta">
        
  
  


  <span class="byline author vcard">Written by <a href="/engineers/ryan-brush"><span class="fn">Ryan Brush</span></a></span>

 | 








  


<time datetime="2013-02-27T00:00:00-06:00" pubdate data-updated="true">Feb 27<span>th</span>, 2013</time>
        
      </div>
    
  </header>


  <div class="entry-content"><h2>From MapReduce to realtime</h2>

<p>This post covers much of the <a href="http://na.apachecon.com/schedule/presentation/161/">Near-Realtime Processing Over HBase</a> talk I’m giving at <a href="http://na.apachecon.com/">ApacheCon NA 2013</a> in blog form. It also draws from the <a href="http://strataconf.com/stratany2012/public/schedule/detail/25387">Hadoop, HBase, and Healthcare</a> talk from StrataConf/Hadoop World 2012.</p>

<p>The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics. For instance, if a user searches for &ldquo;heart disease&rdquo; and a patient has &ldquo;myocardial infarction&rdquo; documented, that document should be highly ranked in the results.</p>

<p>Analyzing and semantically annotating can be computationally expensive, especially when building indexes that could grow into the billions. Algorithms in this space may be discussed in a future blog post, but for now we focus on creation of an infrastructure up to the computational demands. For this, Hadoop is a great fit. A search index is logically a function of a set of input data, and MapReduce allows us to apply such functions in parallel across an arbitrarily large data set.</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/chart-search-screen.png" title="Chart Search" ></p>

<h4>A trend towards competing needs</h4>

<p>The above pattern is powerful but creates a nice problem to have: people want the output of the processing &mdash; in this case, updates to search indexes &mdash; faster. Since we cannot run a MapReduce job over our entire data set every millisecond, we encounter competing needs; the need to <em>process all data holistically</em> conflicts with the need to <em>quickly apply incremental updates</em> to that processing.</p>

<p>This difference may seem simple, but has deep implications.  For instance:</p>

<ul>
<li><p>With MapReduce we can move our computation to the data, but fast updates require moving data to computation.</p></li>
<li><p>MapReduce jobs produce output as a pure function of the input; realtime processing needs to handle outdated state. For instance, we build a phone book and a name changes from Smith to Jones, realtime processing must remove the outdated entry, whereas MapReduce simply rebuilds the whole phone book.</p></li>
<li><p>MapReduce jobs often assume a static set of complete data, whereas realtime processing may see partial data or new data introduce in an unexpected order.</p></li>
</ul>


<p>And despite these differences, our processing output must be the identical; we need to apply the same logic across very different processing models.</p>

<h2>Realtime and batch layers</h2>

<p>These significant differences mean different processing infrastructures. Nathan Marz described this well in his <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">How to Beat the CAP Theorem</a> post. The result is a system that uses complementary technologies: stream-based processing with <a href="http://storm-project.net/" title="">Storm</a> and batch processing with Hadoop.</p>

<p>Interestingly, <a href="http://hbase.apache.org/" title="">HBase</a> sits at a juncture between realtime and batch processing models. It offers aspects of batch processing; computation can be moved to the data via direct MapReduce support. It also supports realtime patterns with random access and fast</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/realtime-layer-batch-layer.png" title="Realtime and Batch Layers" ></p>

<p>reads and writes. So our realtime and batch layers can be viewed like this:</p>

<ol>
<li>Data entering the system is persisted in HBase.</li>
<li>MapReduce jobs are used to create artifacts useful to consumers at scale.</li>
<li>Incremental updates are handled in realtime by processing updates to HBase in a Storm cluster, and are applied to the artifacts produced by MapReduce jobs.</li>
</ol>


<h2>Processing HBase updates in realtime</h2>

<p>So new data lands in HBase but how does Storm know to process it? There is precedent here. Google’s <a href="http://research.google.com/pubs/pub36726.html">Percolator paper</a> describes a technique for doing so over BigTable: it writes a notification entry to a column family whenever a row changes. Processing components scan for notifications and process them as they enter the system.</p>

<p>This is the general approach we have taken to initiate processing in Storm. Google’s Percolator strategy does not translate directly to HBase. Differences in the way regions are managed versus BigTable tables made using a different column family impractical. So we use a separate &ldquo;notification&rdquo; table to track changes to the original.  Updates to HBase go through an API that writes notification entries as well as the data itself. We then wrote a specialized Storm spout that scans the notification table to initiate processing of updates.</p>

<p>The result is processing infrastructure like this, with Storm Spouts and bolts complementing conventional MapReduce processing:</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/processing-diagram.png" title="Processing Diagram" ></p>

<p>The processed data model may be another set of HBase tables, a relational database, or some other data store. Its design should be centered on the needs of the applications and services, letting the processing infrastructure build data for those needs. It is important to note that MapReduce output should be done with a bulk load operation in order to avoid saturating the processed data store with individual updates.</p>

<p>This basic model turns out to be robust. Volume spikes from source systems can be spread throughout the HBase cluster. There are a couple key steps for success here:</p>

<ul>
<li><p>Regular major compactions on the notification HBase tables are essential. Without major compactions, completed notifications will pile up and performance of the system will gradually degrade.</p></li>
<li><p>The notification tables themselves may be small in size, but should be aggressively split across the cluster. This spreads load to handle volume spikes and improve concurrency.</p></li>
</ul>


<p>Also note that MapReduce is still an important part of the system. It’s simply a better tool for batch operations like bringing a new data set online or re-processing an existing data set with new logic.</p>

<h2>Measure Everything</h2>

<p>There are a number of moving parts in this system, and good measurements are the best way to ensure it’s working well. For example, in development we found our HBase Region Servers would encounter frequent but short-lived process queues during heavy load. This didn’t look like an issue in HBase, but when we measured the performance of the calling process there was a noticeable degradation. The point is, instrumentation built into Hadoop and HBase are great but not sufficient. Measuring the observed performance at all layers is important to create an optimal system.</p>

<p>There are many good technologies for doing so. We generally use the <a href="https://github.com/codahale/metrics">Metrics API</a> by Coda Hale. Here is an example of HBase client throughput using an instrumented implementation of HTableInterface. The data is collected by the Metrics API and displayed with <a href="http://graphite.wikidot.com/">Graphite</a>:</p>

<p><img class="center" src="/assets/2013-02-27-near-real-time-processing-over-hadoop-and-hbase/measure-everything.png" title="Measure Everything" ></p>

<h2>Different models, same logic</h2>

<p>The same logic needs to be applied to both batch and stream processing despite the necessary differences in infrastructure. This is a challenge since the models speak very different languages: InputFormats describe an immutable and complete set of data, whereas event streams expose incremental changes without context.</p>

<p>It turns out the function is the only real commonality between them; simply taking a subset of input and returning useful output. So, our strategy is this:</p>

<p><em>Build all logic as a set of simple functions, then compose and coordinate those functions with higher-level processing libraries.</em></p>

<p>We use Storm to compose our realtime processing and Apache Crunch to compose our MapReduce jobs. Here are some lessons we have learned to apply this strategy effectively:</p>

<h4>Minimize intermediate state</h4>

<p>Persisting intermediate state can be expensive and creates complex relationships between moving parts. This is particularly true if a MapReduce job creates intermediate state used by realtime processing or vice versa. Instead, keep processing pipelines independent whenever possible and combine the results at the end.</p>

<h4>Isolate processing models</h4>

<p>Our MapReduce jobs are typically run on separate infrastructure than realtime processing to ensure expensive jobs do not saturate time-critical processing.</p>

<h4>Be aware of the semantic differences in the processing models</h4>

<p>A &ldquo;join&rdquo; in a MapReduce job sees all data, whereas a &ldquo;join&rdquo; in stream processing gets incremental subsets. If a function needs the full context to execute, that context must be externally loaded in the realtime processing system. In our case, external state is loaded from HBase and cached, but projects like <a href="http://engineering.twitter.com/2012/08/trident-high-level-abstraction-for.html">Trident</a> are now providing some aggregation facilities over storm as well.</p>

<h2>The path forward</h2>

<p>The patterns here have been successful but require significant scaffolding and infrastructure to bring together. Near-realtime processing demands over big data are bound to increase, which means there is an opportunity here; higher level abstractions should emerge. Similar to how tools like Crunch and Hive offer abstractions over MapReduce, it’s likely that similar primitives can express the patterns described here.</p>

<p>How these higher abstractions emerge remains to be seen, but there is one thing I’m sure of: it’s going to be fun.</p>

<h2>Acknowledgments</h2>

<p>I’d like to acknowledge key contributors to building this and related systems: Jason Bray, Ben Brown, Robert Farr, Preston Koprivica, Swarnim Kulkarni, Kyle McGovern, Andrew Olson, Mike Richards, Micah Whitacre, Greg Whitsitt, and others.</p>
</div>
  
  


    </article>
  
  <ul class="pager">
    
    <li class="previous"><a href="/blog/page/7/">&larr; Older posts</a></li>
    
    <li><a href="/blog/archives">Blog Archives</a></li>
    
    <li class="next"><a href="/blog/page/5/">Newer posts &rarr;</a></li>
    
  </ul>
</div>
<aside class="sidebar-nav span4" role="complementary">
  
    <section class="well">
  <h2>Recent Posts</h2>
  <ul id="recent_posts" class="nav nav-list">
    
      <li class="post">
        <a href="/blog/managing-30000-logging-events-per-day-with-splunk/">Managing 30,000 Logging Events Per Day With Splunk</a>
      </li>
    
      <li class="post">
        <a href="/blog/cerner-and-the-apache-software-foundation/">Cerner and the Apache Software Foundation</a>
      </li>
    
      <li class="post">
        <a href="/blog/closures-and-currying-in-javascript/">Closures & Currying in JavaScript</a>
      </li>
    
      <li class="post">
        <a href="/blog/intern-hackfest-2014/">Intern HackFest 2014</a>
      </li>
    
      <li class="post">
        <a href="/blog/the-plain-text-is-a-lie/">The Plain Text Is a Lie</a>
      </li>
    
  </ul>
</section>

<section class="well">
  <h2>GitHub Repos</h2>
  <ul id="gh_repos" class="nav">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/cerner">@cerner</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'cerner',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


  
</aside>

    </div>
  </div>
  <footer role="contentinfo" class="page-footer">
  <div class="container">
    <p>&copy; 2015 Cerner</p>
  </div>
</footer>

  










</body>
</html>
